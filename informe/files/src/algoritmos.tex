A partir de los resultados del árbol \textit{simple}, interesa explorar el uso de distintos algoritmos de aprendizaje que puedan arrojar mejores resultados. En particular, evaluaremos la performance de los \textit{árboles de decisión}, \textit{k-nearest neighbours}, \textit{linear discriminant analysis}, \textit{support vector machines} y \textit{gaussian naïve bayes} en términos de \textit{aucroc} promedio. Para cada uno, realizaremos una búsqueda aleatoria con el objetivo de encontrar buenos hiperparámetros, guiados por nuestras propias suposiciones respecto a cuáles pueden llegar a importar y en qué rangos de valores.

La búsqueda se realizará con \textit{RandomizedSearchCV} de \textit{scikit-learn}, utilizando como mecanismo de evaluación $5$-fold cross-validation estratificado. Se harán $100$ iteraciones por algoritmo.

\subsection{Árboles de decisión}
Para este clasificador, se considerarán los siguientes hiperparámetros y rangos. 

\begin{itemize}
    \item El criterio de corte: permitimos todos los criterios implementados por el algoritmo. Estos son \textit{Gini}, \textit{Entropy} y \textit{Log loss}. 
    \item La profundidad máxima: permitimos que varie en el rango $[3,\ \lfloor\sqrt{n} \rfloor]$ de manera uniforme, con $n=451$, bajo la suposición que un árbol \textit{corto} es preferible tanto a un \textit{stump} como a un árbol profundo.
    \item La cantidad de atributos máxima a considerar por corte: permitimos que varíe de manera uniforme sobre $[1, p]$, con $p = 200$.
\end{itemize}

Si bien se exploraron de manera tentativa otros hiperparámetros, se decidió no agregarlos a la búsqueda general, ya que  parecían no influir positivamente en los resultados. Basamos esta idea, también, en que consideramos que la cantidad de restricciones agregadas es inversamente proporcional a la probabilidad de encontrar una buena combinación de hiperparámetros, en especial cuando la influencia de los hiperparámetros no pareceria ser igual. 

La Figura \ref{random_tree} muestra los $5$ mejores candidatos obtenidos. Los resultados respaldan la supocisión de que un árbol corto es mejor a uno profundo (notar que $\lfloor\sqrt{n} \rfloor = 21$).

\vspace{0.5em}
\begin{figure}[!htbp]
    \begin{center}
        \begin{tabular}{ |c|c|c|c| } 
         \hline
        Altura Máxima   & Criterio de corte & Atributos Máximos  & aucroc (validación) \\
        \hline
        $3$             & Entropía          &  $135$            & $0.6843$  \\ 
        $3$             & Entropía          &  $14$             & $0.6598$  \\
        $4$             & Log loss          &  $148$            & $0.6517$  \\ 
        $3$             & Log loss          &  $77$             & $0.6513$  \\
        $9$             & Entropía          &  $91$             & $0.6468$  \\ 
        \hline
        \end{tabular}
    \end{center}
    \caption{mejores resultados para la búsqueda aleatoria de hiperparámetros en el caso de \textit{árboles de decisión}.} \label{random_tree}
\end{figure}

Si bien el criterio de corte influye, la similitud entre Gini y Entropía llevan a intuir que fue la limitación en la cantidad máxima de atributos a considerar la que llevó a una mejora sustancial con respecto al modelo de la Sección \ref{simple}. Esto puede deberse a que limita la influencia de los atributos más importantes durante la construcción del árbol.

\subsection{K-Nearest neighbours}
En este caso, se utilizará el clasificador \textit{KNeighborsClassifier} de \textit{scikit-learn} con las siguientes opciones de hiperparámetros.

\begin{itemize}
    \item Cantidad de vecinos más cercanos: permitimos que sigan una distribución \textit{LogUniform(10, n/2)}. La cota inferior de $10$ porque para valores más bajos no encontramos evidencia empírica que mejore la performance. 
    \item Eleccion de pesos por vecino: Permitimos \textit{Uniform} donde cada instancia se pondera por igual, o \textit{Distance} donde los vecinos más cercanos tendrán un peso más determinante que los demás.
    \item Eleccion de metrica: Optamos que se usen las métricas usuales \textit{l1}, \textit{l2}, \textit{l3} y \textit{l4}. 
\end{itemize}

Se seleccionó una distribución \textit{LogUniforme} para determinar el valor de kk, dado que resulta conveniente evitar valores excesivamente grandes, los cuales podrían inducir overfitting. Al generar 100 muestras de esta distribución y establecer una cota inferior adecuada, se busca controlar el riesgo de underfitting.

Observando la Figura \ref{knn}, se identificó que la elección de pesos ponderados basados en la distancia, combinados con la métrica 1 (l1), pareciera ser óptima para el problema en cuestión.
Además, la búsqueda aleatoria permitió determinar un rango apropiado de vecinos, comprendido entre 10 y 30.
\vspace{0.5em}
\begin{figure}[!htbp]
    \begin{center}
        \begin{tabular}{ |c|c|c|c| } 
         \hline
        N   & Eleccion de pesos & P  & aucroc (validación) \\
        \hline
        $15$             & Distance          &  $1$             & $0.8391$  \\ 
        $11$             & Distance          &  $1$             & $0.8386$  \\
        $14$             & Distance          &  $1$             & $0.8375$  \\ 
        $13$             & Distance          &  $1$             & $0.8360$  \\
        $26$             & Distance          &  $1$             & $0.8287$  \\ 
        \hline
        \end{tabular}
    \end{center}
    \caption{mejores resultados para la búsqueda aleatoria de hiperparámetros para \textit{k-nearest neighbours}.} \label{knn}
\end{figure}

\subsection{Linear discriminant analysis}
La busqueda, para el clasificador \textit{LinearDiscriminantAnalysis} de \textit{scikit-learn} fue llevada a cabo con los siguientes hiperparámetros: 

\begin{itemize}
    \item Solver: Permitimos los algoritmos basados en \textit{LSQR}, \textit{SVD} y \textit{EIGEN}.
    \item El metodo de contraccion: Cuando no se este usando SVD, permitimos que siga una distribucion $Uniforme(0,1)$.
\end{itemize}

Se exploraron otros hiperparámetros, pero optamos por no agregarlos debido a que no parecian influir de forma significativa en los resultados finales. 

Al observar la Figura \ref{lda}, notamos que la elección del Solver (Eigen o Lsqr) parece no influir significativamente. Más bien, resulta más determinante el uso de un parámetro de contracción adecuado, que parece rondar los $0.26$. Finalmente, se observa que el uso de SVD no conduce a una buena performance.

\vspace{0.5em}
\begin{figure}[!htbp]
    \begin{center}
        \begin{tabular}{ |c|c|c|c| } 
         \hline
        Solver   & Shrinkage & auc-roc (validación) \\
        \hline
        eigen                   &  0.270007          & $0.8648$  \\ 
        lsqr                    &  0.272309          & $0.8648$  \\
        lsqr                    &  0.266565          & $0.8648$  \\ 
        lsqr                    &  0.274341          & $0.8647$  \\
        eigen                   &  0.288274          & $0.8645$  \\ 
        \hline
        \end{tabular}
    \end{center}
    \caption{Mejores resultados para la búsqueda aleatoria de hiperparámetros para \textit{linear discriminant analysis}.} \label{lda}
\end{figure}

\subsection{Support vector machines}
Realizamos la busqueda, para el modelo \textit{Support Vector Machines}. Llevada a cabo con el clasificador \textit{SVC} de \textit{scikit-learn}, usamos los siguientes hiperparámetros y rangos: 

\begin{itemize}
    \item Parámetro de regularización: Optamos porque siga una distribucion \textit{LogUniform(1e-5,1e2)}, como el valor es inversamente proporcional a `C` es relevante que tome mas valores chicos que grandes pues hara que la suma poderada de errores de las instancias que estan en el lado equivocado de la frontera de decision, tenga un grado mas de tolerancia.
    \item Kernel: Permitimos \textit{linear}, \textit{poly}, \textit{rbf} (función de base radial) y \textit{sigmoid} (función tanh).
    \item Coeficiente de Kernel: Decidimos elegir entre \textit{scale}, que toma $ \gamma = \frac{1}{p*X.var()}$, u \textit{auto} con $\gamma = \frac{1}{p} $, donde $p=200$ (cantidad de genes).
\end{itemize}

En la Figura \ref{support vector machines}, se observa que la configuración más relevante, a priori, es que $\gamma$ este escalado, que busca normalizar la influencia de las características en el cálculo de la función de kernel. Esta normalización es especialmente útil para conjuntos de datos que contienen características de diferentes escalas. Además, se destaca que el kernel de función de base radial resultó ser el más exitoso. Aunque no parece que el parámetro C tenga una influencia significativa, los valores seleccionados no fueron muy grandes. Es importante señalar que los grados solo se consideran en los kernels \textit{poly}; por lo tanto, no tienen relevancia para los demás tipos de kernel.
\vspace{0.5em}
\begin{figure}[!htbp]
    \begin{center}
        \begin{tabular}{ |c|c|c|c|c| } 
         \hline
        $C$ & Kernel & $\gamma$ & grado & aucroc (validación) \\
        \hline
        $2.5408$ & rbf     & scale & $7$ & $0.8960$ \\ 
        $2.6202$ & rbf    & scale & $6$ & $0.8960$ \\
        $13.4965$ & rbf   & scale & $3$ & $0.8956$ \\ 
        $20.2985$ & rbf   & scale & $8$ & $0.8956$ \\
        $3.4142$ & rbf    & scale & $6$ & $0.8955$ \\ 
        \hline
        \end{tabular}
    \end{center}
    \caption{mejores resultados para la búsqueda aleatoria de hiperparámetros para \textit{support vector machines}.} \label{support vector machines}
\end{figure}

\subsection{Gaussian naïve bayes}
Se examinó por último el rendimiento del clasificador \textit{GaussianNB} de \textit{scikit-learn}. Los hiperparámetros de este algoritmo a optimizar y sus rangos fueron:

\begin{itemize}
    \item Las probabilidades a priori de las clases: permitimos que varíen de manera normal con \textit{media} igual a las probabilidades a priori empíricas (ver Figura \ref{distribucion}) y una \textit{desviación estándar} de $\sigma = 0.1$.
    \item El suavizado de varianza: permitimos que varíe uniformemente en el rango $[0, 1 \times 10^{-2}]$.
\end{itemize}

Como valor predeterminado, \textit{GaussianNB} considera las \textit{probabilidades a priori} de cada clase como sus respectivas proporciones en el dataset. Para ampliar el rango de búsqueda, en lugar de usar las probabilidades originales se utilizaron valores aleatorios cercanos a estas.

Por otro lado, el \textit{suavizado de varianza} indica la cantidad que se suma a todas las varianzas de los atributos para evitar que sean cero\footnote{Su valor representa el porcentaje de la varianza más alta a sumarle a todas las demás.}, lo cual podría producir errores numéricos durante el cálculo de las distribuciones normales. Aunque tiene un valor predeterminado de $1 \times 10^{-9}$, probamos con números al azar hasta $1 \times 10^{-2}$.

Al observar la Figura \ref{naive_bayes}, podemos destacar dos hallazgos interesantes. En primer lugar, parece que el algoritmo tiene baja varianza, ya que al cambiar significativamente las proporciones de las etiquetas su capacidad predictiva se mantiene estable. Por otro lado, da la impresión de que el mejor valor para el suavizado de la varianza de los atributos se encuentra cerca de la cota superior de la búsqueda, es decir, $1 \times 10^{-2}$. Al dejar su valor predeterminado de $1 \times 10^{-9}$, el mejor puntaje obtenido fue de $0.785$, aproximadamente un $7\%$ de reducción respecto a los de la tabla.

\vspace{0.5em}
\begin{figure}[!htbp]
    \begin{center}
        \begin{tabular}{ |c|c|c|c| } 
         \hline
        $P(Y=0)$ & $P(Y=1)$ & Suavizado & aucroc (validación) \\
        \hline
        $0.6320$ & $0.3680$ & $0.975 \times 10^{-2}$ & $0.8408$ \\ 
        $0.6119$ & $0.3881$ & $0.973 \times 10^{-2}$ & $0.8408$  \\
        $0.6579$ & $0.3421$ & $0.982 \times 10^{-2}$ & $0.8406$  \\ 
        $0.8183$ & $0.1817$ & $0.980 \times 10^{-2}$ & $0.8406$  \\
        $0.5157$ & $0.4843$ & $0.960 \times 10^{-2}$ & $0.8406$  \\ 
        \hline
        \end{tabular}
    \end{center}
    \caption{mejores resultados para la búsqueda aleatoria de hiperparámetros para \textit{naïve bayes}.} \label{naive_bayes}
\end{figure}

