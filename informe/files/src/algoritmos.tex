Procedemos a evaluar el uso de distintos algoritmos de aprendizaje supervisado para la resolución del problema. Para cada uno, realizaremos una búsqueda aleatoria con el objetivo de encontrar buenos hiperparámetros, guiados por nuestras propias suposiciones respecto de cuáles pueden llegar a importar y en qué rangos de valores.

La búsqueda se realizará con \textit{RandomizedSearchCV} de \textit{scikit-learn}, utilizando como mecanismo de evaluación $5$-fold cross-validation estratificado con métrica \textit{aucroc} promedio. Se realizarán $100$ iteraciones por algoritmo.

\subsection{decision trees}
Realizamos una última búsqueda de hiperparámetros prometedores para los \textit{árboles de decisión}. La misma se llevó a cabo con el clasificador \textit{DecisionTreeClassifier} de \textit{scikit-learn} para los siguientes hiperparámetros y rangos: 

\begin{itemize}
    \item El criterio de corte: permitimos todos los criterios implementados por el algoritmo. Estos son \textit{Gini}, \textit{Entropy} y \textit{Log loss}.
    \item La profundidad máxima: permitimos que varie de manera uniforme en el rango $[3.\ \lfloor\sqrt{n} \rfloor]$, con $n=451$ la cantidad de instancias en $D_{train}$, bajo la suposición que un árbol \textit{corto} es preferible tanto a un \textit{stump} como a un árbol profundo. 
    \item La cantidad de atributos máxima a considerar por corte: permitimos que varíe de manera uniforme sobre $[1, p]$, con $p = 200$ la cantidad total de atributos en $D_{train}$.
\end{itemize}

Si bien se exploraron de manera tentativa otros hiperparámetros, se decidió no agregarlos a la búsqueda general, ya que no parecieron influir positivamente en los resultados. A su vez, consideramos que la cantidad de restricciones agregadas es inversamente proporcional a la probabilidad de encontrar una buena combinación de hiperparámetros, en especial cuando la influencia de los hiperparámetros no parece ser igual. 

La Figura \ref{random_tree} muestra los $5$ mejores candidatos obtenidos. Los resultados parecen respaldar la supocisión de que un árbol corto es mejor (notar que $\lfloor\sqrt{n} \rfloor = 21$).

\vspace{0.5em}
\begin{figure}[!htbp]
    \begin{center}
        \begin{tabular}{ |c|c|c|c| } 
         \hline
        Altura Máxima   & Criterio de corte & Atributos Máximos  & aucroc (validación) \\
        \hline
        $3$             & Entropía          &  $135$            & $0.6843$  \\ 
        $3$             & Entropía          &  $14$             & $0.6598$  \\
        $4$             & Log loss          &  $148$            & $0.6517$  \\ 
        $3$             & Log loss          &  $77$             & $0.6513$  \\
        $9$             & Entropía          &  $91$             & $0.6468$  \\ 
        \hline
        \end{tabular}
    \end{center}
    \caption{mejores resultados para la búsqueda aleatoria de hiperparámetros para \textit{árboles de decisión}.} \label{random_tree}
\end{figure}

Si bien el criterio de corte influye, la similitud entre Gini y Entropía llevan a intuir que fue la limitación en la cantidad máxima de atributos a considerar la que llevó a una mejora sustancial con respecto al modelo de la Sección \ref{simple}. Esto puede deberse a que limita la influencia de los atributos más importantes durante la construcción del árbol.

\subsection{k-nearest neighbours}
\subsection{linear discriminant analysis}
\subsection{support vector machines}
\subsection{naïve bayes}