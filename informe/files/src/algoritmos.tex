A partir de los resultados del árbol \textit{simple}, interesa explorar el uso de distintos algoritmos de aprendizaje que puedan arrojar mejores resultados. En particular, evaluaremos la performance de los \textit{árboles de decisión}, \textit{k-nearest neighbours}, \textit{linear discriminant analysis}, \textit{support vector machines} y \textit{gaussian naïve bayes} en términos de \textit{aucroc} promedio. Para cada uno, realizaremos una búsqueda aleatoria con el objetivo de encontrar buenos hiperparámetros, guiados por nuestras propias suposiciones respecto a cuáles pueden llegar a importar y en qué rangos de valores.

La búsqueda se realizará con \textit{RandomizedSearchCV} de \textit{scikit-learn}, utilizando como mecanismo de evaluación $5$-fold cross-validation estratificado. Se harán $100$ iteraciones por algoritmo.

\subsection{Árboles de decisión}
Para este clasificador, se considerarán los siguientes hiperparámetros y rangos. 

\begin{itemize}
    \item El criterio de corte: permitimos todos los criterios implementados por el algoritmo. Estos son \textit{Gini}, \textit{Entropy} y \textit{Log loss}. 
    \item La profundidad máxima: permitimos que varíe en el rango $[3,\ \lfloor\sqrt{n} \rfloor]$ de manera uniforme, con $n=449$, bajo la suposición que un árbol \textit{corto} es preferible tanto a un \textit{stump} como a un árbol profundo.
    \item La cantidad de atributos máxima a considerar por corte: permitimos que varíe de manera uniforme sobre $[1, p]$, con $p = 200$.
\end{itemize}

Si bien se exploraron de manera tentativa otros hiperparámetros, se decidió no agregarlos a la búsqueda general, ya que parecían no influir positivamente en los resultados. Basamos esta idea, también, en que consideramos que la cantidad de restricciones agregadas es inversamente proporcional a la probabilidad de encontrar una buena combinación de hiperparámetros, en especial cuando la influencia de los hiperparámetros no parece ser igual. 

La Figura \ref{random_tree} muestra los $5$ mejores candidatos obtenidos. Los resultados respaldan la suposición que un árbol corto es mejor a uno profundo (notar que $\lfloor\sqrt{n} \rfloor = 21$).

\vspace{0.5em}
\begin{figure}[!htbp]
    \begin{center}
        \begin{tabular}{ |c|c|c|c| } 
         \hline
        Altura Máxima   & Criterio de corte & Atributos Máximos  & aucroc (validación) \\
        \hline
        $3$             & Entropía          &  $135$            & $0.6843$  \\ 
        $3$             & Entropía          &  $14$             & $0.6598$  \\
        $4$             & Log loss          &  $148$            & $0.6517$  \\ 
        $3$             & Log loss          &  $77$             & $0.6513$  \\
        $9$             & Entropía          &  $91$             & $0.6468$  \\ 
        \hline
        \end{tabular}
    \end{center}
    \caption{mejores resultados para la búsqueda aleatoria de hiperparámetros en el caso de \textit{árboles de decisión}.} \label{random_tree}
\end{figure}

Si bien el criterio de corte influye, la similitud entre Gini y Entropía llevan a intuir que fue la limitación en la cantidad máxima de atributos a considerar la que llevó a una mejora sustancial con respecto al modelo de la Sección \ref{simple}. Esto puede deberse a que limita la influencia de los atributos más importantes durante la construcción del árbol.

\subsection{K-Nearest neighbours}
En este caso, se utilizará el clasificador \textit{KNeighborsClassifier} de \textit{scikit-learn} con las siguientes opciones de hiperparámetros.

\begin{itemize}
    \item Cantidad de vecinos más cercanos: permitimos que sigan una distribución \textit{logUniforme} en el rango $[10, n/2]$.
    \item Eleccion de pesos por vecino: permitimos las opciones \textit{Uniform} ---donde cada instancia se pondera por igual---, y \textit{Distance} ---donde los vecinos más cercanos tienen un peso más determinante que los demás---.
    \item Eleccion de metrica: optamos que se usen las métricas usuales \textit{l1}, \textit{l2}, \textit{l3} y \textit{l4}. 
\end{itemize}

Se seleccionó una distribución \textit{LogUniforme} ya que resulta conveniente evitar valores excesivamente grandes, los cuales podrían inducir un sobreajuste. Del mismo modo, la cota inferior busca controlar el riesgo de subajuste. El valor de $10$ se obtuvo a prueba y error. Al intentar con valores más bajos, no notamos mejoras en la performance. 

La Figura \ref{knn} muestra los resultados. La elección de pesos ponderados basados en la distancia, combinados con la métrica $l1$, parecen dar buenos resultados. Además, la búsqueda aleatoria permitió determinar un rango apropiado de vecinos, comprendido entre $10$ y $30$.

Vale la pena mencionar que se repitió este experimento con distintos tipos de normalización y estandarización de los datos. Se probó transformar los datos con los preprocesadores \textit{StandardScaler}, \textit{MaxAbsScaler} y \textit{RobustScaler} de \textit{scikit-learn} para evitar los problemas de escalas. Sin embargo, sus resultados fueron peores, por lo que optamos por no incluirlos.
  
\vspace{0.5em}
\begin{figure}[!htbp]
    \begin{center}
        \begin{tabular}{ |c|c|c|c| } 
         \hline
        Cantidad de vecinos & Elección de pesos & Métrica & aucroc (validación) \\
        \hline
        $15$             & Distance          & \textit{l1} & $0.8391$  \\ 
        $11$             & Distance          & \textit{l1} & $0.8386$  \\
        $14$             & Distance          & \textit{l1} & $0.8376$  \\ 
        $13$             & Distance          & \textit{l1} & $0.8361$  \\
        $26$             & Distance          & \textit{l1} & $0.8288$  \\ 
        \hline
        \end{tabular}
    \end{center}
    \caption{mejores resultados para la búsqueda aleatoria de hiperparámetros en el caso de \textit{k-nearest neighbours}.} \label{knn}
\end{figure}

\subsection{Linear discriminant analysis}
Utilizaremos el clasificador \textit{LinearDiscriminantAnalysis} de \textit{scikit-learn} con los siguientes hiperparámetros. 

\begin{itemize}
    \item Solver: permitimos las implementaciones basadas en \textit{lsqr}, \textit{svd} y \textit{eigen} provistas por el clasificador.
    \item Método de contracción: para las implementaciones de \textit{lsqr} y \textit{eigen}, permitimos que varíe en el rango $[0, 1]$ de manera uniforme.
\end{itemize}

% Se exploraron otros hiperparámetros, pero optamos por no agregarlos debido a que no parecian influir de forma significativa en los resultados finales. 

La Figura \ref{lda} muestra los resultados. Notamos que la elección del Solver no parece influir significativamente en los casos de \textit{eigen} y \textit{lsqr}, pero sí en el caso de \textit{svd}, donde la performance fue marcadamente peor. En base a esto, consideramos más importante el uso de un método de contracción adecuado.

\vspace{0.5em}
\begin{figure}[!htbp]
    \begin{center}
        \begin{tabular}{ |c|c|c|c| } 
         \hline
        Solver   & Shrinkage & auc-roc (validación) \\
        \hline
        eigen                   &  0.270007          & $0.8649$  \\ 
        lsqr                    &  0.272309          & $0.8649$  \\
        lsqr                    &  0.266565          & $0.8649$  \\ 
        lsqr                    &  0.274341          & $0.8648$  \\
        eigen                   &  0.288274          & $0.8646$  \\ 
        \hline
        \end{tabular}
    \end{center}
    \caption{Mejores resultados para la búsqueda aleatoria de hiperparámetros en el caso de \textit{linear discriminant analysis}.} \label{lda}
\end{figure}

\subsection{Support vector machines} En este caso, buscaremos buenos hiperparámetros para el clasificador \textit{SVC} de \textit{scikit-learn}. Consideramos los siguientes parámetros y rangos. 

\begin{itemize}
    \item C: permitimos una distribución $\textit{logUniforme}$ en el rango $[1\times10^{-5}, 1\times10^{2}]$. Este parámetro es inversamente proporcional a la tolerancia del modelo respecto a los errores de clasificación.
    \item Kernel: permitimos los kernels \textit{linear}, \textit{poly}, \textit{rbf} (función de base radial) y \textit{sigmoid} (función tanh) provistos por el algoritmo.
    \item Coeficiente de kernel: decidimos elegir entre \textit{scale}, que toma $\gamma = 1/(p\cdot\text{Var}[D_{train}])$, y \textit{auto}, con $\gamma = 1/p$.
    \item Grado: permitimos que varíe de manera uniforme en el rango $[2, 10]$. Este hiperparámetro solo se utiliza en el kernel \textit{poly}.
\end{itemize}

La Figura \ref{svm} muestra los resultados. Se observa que la configuración más relevante, a priori, corresponde a $\gamma$ escalado. Este hiperparámetro busca normalizar la influencia de las instancias en el cálculo de la función del kernel. Esta normalización es especialmente útil para conjuntos de datos que contienen instancias de diferentes escalas, como es el caso de $D_{train}$. Además, se destaca que el kernel de función de base radial resultó ser el más exitoso. 

Aunque no parezca que el parámetro C tenga una influencia significativa, los valores seleccionados no fueron muy grandes, por lo que se probaron modelos más permisivos, lo que podría ayudar a reducir las chances de sobreajuste del modelo. 

Es importante señalar, también, que como los grados sólo se consideran en los kernels \textit{poly}, no tienen relevancia para los demás tipos de kernel.

\vspace{0.5em}
\begin{figure}[!htbp]
    \begin{center}
        \begin{tabular}{ |c|c|c|c|c| } 
         \hline
        $C$ & Kernel & $\gamma$ & grado & aucroc (validación) \\
        \hline
        $2.5408$ & rbf     & scale & $7$ & $0.8961$ \\ 
        $2.6202$ & rbf    & scale & $6$ & $0.8961$ \\
        $13.4965$ & rbf   & scale & $3$ & $0.8956$ \\ 
        $20.2985$ & rbf   & scale & $8$ & $0.8956$ \\
        $3.4142$ & rbf    & scale & $6$ & $0.8956$ \\ 
        \hline
        \end{tabular}
    \end{center}
    \caption{mejores resultados para la búsqueda aleatoria de hiperparámetros en el caso de \textit{support vector machines}.} \label{svm}
\end{figure}

\subsection{Gaussian naïve bayes}
Se examinará por último el rendimiento del clasificador \textit{GaussianNB} de \textit{scikit-learn}. Los hiperparámetros de este algoritmo a optimizar y sus rangos son:

\begin{itemize}
    \item Las probabilidades a priori de las clases: permitimos que varíen de manera normal con \textit{media} igual a las probabilidades a priori empíricas (ver Figura \ref{distribucion}) y una \textit{desviación estándar} de $\sigma = 0.1$.
    \item El suavizado de varianza: permitimos que varíe uniformemente en el rango $[0,\ 1 \times 10^{-2}]$.
\end{itemize}

Como valor predeterminado, \textit{GaussianNB} considera las \textit{probabilidades a priori} de cada clase como sus respectivas proporciones en el dataset. Para ampliar el rango de búsqueda, en lugar de usar las probabilidades originales, se utilizaron valores aleatorios cercanos a estas.

Por otro lado, el \textit{suavizado de varianza} indica la cantidad que se suma a todas las varianzas de los atributos para evitar que sean cero\footnote{Su valor representa el porcentaje de la varianza más alta a sumarle a todas las demás.}, lo cual podría producir errores numéricos durante el cálculo de las distribuciones normales. Aunque tiene un valor predeterminado de $1 \times 10^{-9}$, probamos con números al azar hasta $1 \times 10^{-2}$.

Al observar la Figura \ref{naive_bayes}, podemos destacar dos hallazgos interesantes. En primer lugar, parece que el algoritmo tiene baja varianza, ya que al cambiar significativamente las proporciones de las etiquetas su capacidad predictiva se mantiene estable. Por otro lado, da la impresión de que el mejor valor para el suavizado de la varianza de los atributos se encuentra cerca de la cota superior de la búsqueda, es decir, $1 \times 10^{-2}$. Al dejar su valor predeterminado de $1 \times 10^{-9}$, el mejor puntaje obtenido fue de $0.785$, aproximadamente un $7\%$ de reducción respecto a los de la tabla.

\vspace{0.5em}
\begin{figure}[!htbp]
    \begin{center}
        \begin{tabular}{ |c|c|c|c| } 
         \hline
        $P(Y=0)$ & $P(Y=1)$ & Suavizado & aucroc (validación) \\
        \hline
        $0.6320$ & $0.3680$ & $0.975 \times 10^{-2}$ & $0.8408$ \\ 
        $0.6119$ & $0.3881$ & $0.973 \times 10^{-2}$ & $0.8408$  \\
        $0.6579$ & $0.3421$ & $0.982 \times 10^{-2}$ & $0.8406$  \\ 
        $0.8183$ & $0.1817$ & $0.980 \times 10^{-2}$ & $0.8406$  \\
        $0.5157$ & $0.4843$ & $0.960 \times 10^{-2}$ & $0.8406$  \\ 
        \hline
        \end{tabular}
    \end{center}
    \caption{mejores resultados para la búsqueda aleatoria de hiperparámetros en el caso de \textit{naïve bayes}.} \label{naive_bayes}
\end{figure}

