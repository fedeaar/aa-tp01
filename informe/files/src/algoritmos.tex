Procedemos a evaluar el uso de distintos algoritmos de aprendizaje supervisado para la resolución del problema. Para cada uno, realizaremos una búsqueda aleatoria con el objetivo de encontrar buenos hiperparámetros, guiados por nuestras propias suposiciones respecto de cuáles pueden llegar a importar y en qué rangos de valores.

La búsqueda se realizará con \textit{RandomizedSearchCV} de \textit{scikit-learn}, utilizando como mecanismo de evaluación $5$-fold cross-validation estratificado con métrica \textit{aucroc} promedio. Se realizarán $100$ iteraciones por algoritmo.

\subsection{decision trees}
Realizamos una última búsqueda de hiperparámetros prometedores para los \textit{árboles de decisión}. La misma se llevó a cabo con el clasificador \textit{DecisionTreeClassifier} de \textit{scikit-learn} para los siguientes hiperparámetros y rangos: 

\begin{itemize}
    \item El criterio de corte: permitimos todos los criterios implementados por el algoritmo. Estos son \textit{Gini}, \textit{Entropy} y \textit{Log loss}. 
    \item La profundidad máxima: permitimos que varie en el rango $[3,\ \lfloor\sqrt{n} \rfloor]$ de manera uniforme, con $n=451$ la cantidad de instancias en $D_{train}$, bajo la suposición que un árbol \textit{corto} es preferible tanto a un \textit{stump} como a un árbol profundo.
    \item La cantidad de atributos máxima a considerar por corte: permitimos que varíe de manera uniforme sobre $[1, p]$, con $p = 200$ la cantidad total de atributos en $D_{train}$.
\end{itemize}

Si bien se exploraron de manera tentativa otros hiperparámetros, se decidió no agregarlos a la búsqueda general, ya que no parecieron influir positivamente en los resultados. A su vez, consideramos que la cantidad de restricciones agregadas es inversamente proporcional a la probabilidad de encontrar una buena combinación de hiperparámetros, en especial cuando la influencia de los hiperparámetros no parece ser igual. 

La Figura \ref{random_tree} muestra los $5$ mejores candidatos obtenidos. Los resultados parecen respaldar la supocisión de que un árbol corto es mejor (notar que $\lfloor\sqrt{n} \rfloor = 21$).

\vspace{0.5em}
\begin{figure}[!htbp]
    \begin{center}
        \begin{tabular}{ |c|c|c|c| } 
         \hline
        Altura Máxima   & Criterio de corte & Atributos Máximos  & aucroc (validación) \\
        \hline
        $3$             & Entropía          &  $135$            & $0.6843$  \\ 
        $3$             & Entropía          &  $14$             & $0.6598$  \\
        $4$             & Log loss          &  $148$            & $0.6517$  \\ 
        $3$             & Log loss          &  $77$             & $0.6513$  \\
        $9$             & Entropía          &  $91$             & $0.6468$  \\ 
        \hline
        \end{tabular}
    \end{center}
    \caption{mejores resultados para la búsqueda aleatoria de hiperparámetros para \textit{árboles de decisión}.} \label{random_tree}
\end{figure}

Si bien el criterio de corte influye, la similitud entre Gini y Entropía llevan a intuir que fue la limitación en la cantidad máxima de atributos a considerar la que llevó a una mejora sustancial con respecto al modelo de la Sección \ref{simple}. Esto puede deberse a que limita la influencia de los atributos más importantes durante la construcción del árbol.

\subsection{k-nearest neighbours}
\subsection{linear discriminant analysis}
La busqueda de hiperparametros se llevo a cabo con el clasificador \textit{LinearDiscriminantAnalysis} de \textit{scikit-learn} usamos los siguientes hiperparámetros: 

\begin{itemize}
    \item Solver: Permitimos los algoritmos basados en $LSQR$, $SVD$ y $EIGEN$.
    \item El metodo de contraccion: Cuando no se este usando Solver=SVD, permitimos que siga una distribucion $Uniforme(0,1)$.
\end{itemize}

Se exploraron otros hiperparámetros, pero optamos por no agregarlos debido a que no parecian influir de forma significativa en los resultados finales. 

\vspace{0.5em}
\begin{figure}[!htbp]
    \begin{center}
        \begin{tabular}{ |c|c|c|c| } 
         \hline
        Solver   & Shrinkage & auc-roc (validación) \\
        \hline
        eigen                   &  0.270007          & $0.8648$  \\ 
        lsqr                    &  0.272309          & $0.8648$  \\
        lsqr                    &  0.266565          & $0.8648$  \\ 
        lsqr                    &  0.274341          & $0.8647$  \\
        eigen                   &  0.288274          & $0.8645$  \\ 
        \hline
        \end{tabular}
    \end{center}
    \caption{Mejores resultados para la búsqueda aleatoria de hiperparámetros para \textit{Linear Discriminant Analysis}.} \label{random_tree}
\end{figure}

La eleccion del Solver Eigen o Lsqr, pareciera no influir pues es mas determinante usar un parametro de contraccion, el uso de svd da como resultado una peor performance. 
\subsection{support vector machines}

\subsection{naïve bayes}
Se examinó por último el rendimiento del clasificador \textit{GaussianNB} de \textit{scikit-learn}. Los hiperparámetros de este algoritmo a optimizar y sus rangos fueron:

\begin{itemize}
    \item Las probabilidades a priori de las clases: permitimos que varíen de manera normal con \textit{media} igual a las probabilidades a priori empíricas (ver Figura \ref{distribucion}) y una \textit{desviación estándar} de $\sigma = 0.1$.
    \item El suavizado de varianza: permitimos que varíe uniformemente en el rango $[0, 1 \times 10^{-2}]$.
\end{itemize}

Como valor predeterminado, \textit{GaussianNB} considera las \textit{probabilidades a priori} de cada clase como sus respectivas proporciones en el dataset. Para ampliar el rango de búsqueda, en lugar de usar las probabilidades originales se utilizaron valores aleatorios cercanos a estas.

Por otro lado, el \textit{suavizado de varianza} indica la cantidad que se suma a todas las varianzas de los atributos para evitar que sean cero\footnote{Su valor representa el porcentaje de la varianza más alta a sumarle a todas las demás.}, lo cual podría producir errores numéricos durante el cálculo de las distribuciones normales. Aunque tiene un valor predeterminado de $1 \times 10^{-9}$, probamos con números al azar hasta $1 \times 10^{-2}$.

Al observar la Figura \ref{naive_bayes}, podemos destacar dos hallazgos interesantes. En primer lugar, parece que el algoritmo tiene baja varianza, ya que al cambiar significativamente las proporciones de las etiquetas su capacidad predictiva se mantiene estable. Por otro lado, da la impresión de que el mejor valor para el suavizado de la varianza de los atributos se encuentra cerca de la cota superior de la búsqueda, es decir, $1 \times 10^{-2}$. Al dejar su valor predeterminado de $1 \times 10^{-9}$, el mejor puntaje obtenido fue de $0.785$, aproximadamente un $7\%$ de reducción respecto a los de la tabla.

\vspace{0.5em}
\begin{figure}[!htbp]
    \begin{center}
        \begin{tabular}{ |c|c|c|c| } 
         \hline
        $P(Y=0)$ & $P(Y=1)$ & Suavizado & aucroc (validación) \\
        \hline
        $0.6320$ & $0.3680$ & $0.975 \times 10^{-2}$ & $0.8408$ \\ 
        $0.6119$ & $0.3881$ & $0.973 \times 10^{-2}$ & $0.8408$  \\
        $0.6579$ & $0.3421$ & $0.982 \times 10^{-2}$ & $0.8406$  \\ 
        $0.8183$ & $0.1817$ & $0.980 \times 10^{-2}$ & $0.8406$  \\
        $0.5157$ & $0.4843$ & $0.960 \times 10^{-2}$ & $0.8406$  \\ 
        \hline
        \end{tabular}
    \end{center}
    \caption{mejores resultados para la búsqueda aleatoria de hiperparámetros para \textit{naïve bayes}.} \label{naive_bayes}
\end{figure}

